[![Torch Version](https://img.shields.io/badge/torch>=1.10.0-61DAFB.svg?style=flat-square)](#torch) [![Torchvision Version](https://img.shields.io/badge/torchvision>=0.2.2-yellow.svg?style=flat-square)](#torchvision) [![Python Version](https://img.shields.io/badge/python->=3.6-blue.svg?style=flat-square)](#python) 

## Installation
```
git clone https://github.com/kkugosu/RL_BASIC.git
```
## Experiment
```
python executable.py
```
you can choose belows
* environments 
* policy 
* hiddenlayer size
* batch size
* memory capacity
* memory reset time
* train time per memory
* learning rate
* eligibility trace
* done penalty
* load previous model or not (not = 0 yes = 1)

## Example

![ezgif com-gif-maker(1)](https://user-images.githubusercontent.com/24292848/181578863-d4aea7cc-a683-46c5-a564-a75969065263.gif)

result of reward > 500

## Experiment environment

* cartpole
* hopper


## Requirements

* Gym
* Mujoco
* Python >= 3.8 
* Pytorch >= 1.12.0
* Numpy


## Easy explained algorithm principle

> [policy gradient](https://github.com/kkugosu/RL_BASIC/blob/master/docs/pg.md)
>
> [deep queue network](https://github.com/kkugosu/RL_BASIC/blob/master/docs/dqn.md)
>
> [actor critic](https://github.com/kkugosu/RL_BASIC/blob/master/docs/ac.md)
>
> [deep deterministic policy gradient](https://github.com/kkugosu/RL_BASIC/blob/master/docs/ddpg.md)
>
> [Trust region policy optimization](https://github.com/kkugosu/RL_BASIC/blob/master/docs/trpo.md)
>
> [proximal policy optimization](https://github.com/kkugosu/RL_BASIC/blob/master/docs/ppo.md)
>
> [soft actor critic](https://github.com/kkugosu/RL_BASIC/blob/master/docs/sac.md)

## REPO

* papers

* cs285

* hui reinforcement learning blog
